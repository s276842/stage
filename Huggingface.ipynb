{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4d848a2",
   "metadata": {},
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31912945",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "## Sequence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71dc2a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7eef2879f54a88b3ab0618962e569f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66579c85646a42058d4ff1a3eeb03040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/255M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e68ff9961a4019836b49d4e8d96b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad4eefd655445768169c2f122525ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b28bf28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9991129040718079}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = classifier('I hate you')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e049c3",
   "metadata": {},
   "source": [
    "> paraphrase classification with BERT pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c30439ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a689d15de49d4f6db3b01004c5ac8ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c5e195405b481b9fe65db34157fce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4606d51f53547968bd470b35d2cbdbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8082f52fb7514696948b4d82c2ea2481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551a3dd9bfa54f1a8f32b29560c14474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/413M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "MODEL_PATH = \"bert-base-cased-finetuned-mrpc\"\n",
    "\n",
    "# 1. Instantiate tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "\n",
    "classes = ['not paraphrase', 'is paraphrase']\n",
    "\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "\n",
    "# 2. tokenize the sentences\n",
    "# The tokenizer will automatically add any model specific separators (i.e. <CLS> and <SEP>) and tokens to\n",
    "# the sequence, as well as compute the attention masks.\n",
    "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\n",
    "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d62253c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1109,  1419, 20164, 10932,  2271,  7954,  1110,  1359,  1107,\n",
       "          1203,  1365,  1392,   102, 20164, 10932,  2271,  7954,   112,   188,\n",
       "          3834,  1132,  3629,  1107,  6545,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a28046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. retrieve the logits\n",
    "paraphrase_classification_logits = model(**paraphrase).logits\n",
    "not_paraphrase_classification_logits = model(**not_paraphrase).logits\n",
    "\n",
    "# 4. retrieve probabilities through softmax\n",
    "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9315a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O:  The company HuggingFace is based in New York City\n",
      "P:  HuggingFace's headquarters are situated in Manhattan\n",
      "not paraphrase: 9.54%\n",
      "is paraphrase: 90.46%\n",
      "----------\n",
      "O:  The company HuggingFace is based in New York City\n",
      "P:  Apples are especially bad for your health\n",
      "not paraphrase: 94.04%\n",
      "is paraphrase: 5.96%\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "def print_res(classes, prob):\n",
    "    for c, p in zip(classes, prob):\n",
    "        print(f\"{c}: {p*100:.2f}%\")\n",
    "\n",
    "#paraphrase\n",
    "print(\"O: \", sequence_0)\n",
    "print(\"P: \", sequence_2)\n",
    "print_res(classes, paraphrase_results)\n",
    "print(\"-\"*10)\n",
    "\n",
    "#not paraphrase\n",
    "print(\"O: \", sequence_0)\n",
    "print(\"P: \", sequence_1)\n",
    "print_res(classes, not_paraphrase_results)\n",
    "print(\"-\"*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf82d7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09536290913820267, 0.9046370387077332]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0897e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0954, 0.9046]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(paraphrase_classification_logits, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2829cf4b",
   "metadata": {},
   "source": [
    "## Extractive Question Answering\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune a model on a SQuAD task, you may leverage the run_qa.py and run_tf_squad.py scripts.\n",
    "\n",
    "### From Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e7234dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a question answering dataset is the   SQuAD dataset, which is entirely based on that task. If you would like to fine-tune a model on a SQuAD task, you may leverage the   examples/pytorch/question-answering/run_squad.py script.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d20aa5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = r\"\"\"Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a question answering dataset is the \n",
    "            SQuAD dataset, which is entirely based on that task. If you would like to fine-tune a model on a SQuAD task, you may leverage the \n",
    "            examples/pytorch/question-answering/run_squad.py script.\"\"\"\n",
    "context = re.sub('\\n', ' ', context)\n",
    "context = re.sub(' +', ' ', context)\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2ec69ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.6177273988723755,\n",
       " 'start': 33,\n",
       " 'end': 94,\n",
       " 'answer': 'the task of extracting an answer from a text given a question'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = question_answerer(question=\"What is extractive question answering?\", context=context)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "879090d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'SQuAD dataset', score: 0.5152, start: 146, end: 159\n"
     ]
    }
   ],
   "source": [
    "result = question_answerer(question=\"What is a good example of a question answering dataset?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50066f5c",
   "metadata": {},
   "source": [
    "Note how the model is doing 'simply' an heuristic search in the text. Given a quesiton like \"On what is based the SQuAD dataset?\" the correct answer would be something like \"It is based on the task of Extractive Question Answering\". However, this requires a more complex reasonament on the text. \n",
    ">This model simply reduce the task to find a sub-span of the text that likely answers the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "869f62d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.6470518708229065, 'start': 188, 'end': 197, 'answer': 'that task'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = question_answerer(question=\"On what is based the SQuAD dataset?\", context=context)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe74035",
   "metadata": {},
   "source": [
    "### From AutoModel\n",
    "As in the previous case we can manually manage tokenizers and models. This requires to:\n",
    "1. Instantite tokenizer and model from a checkpoint name\n",
    "2. Define Context and Questions\n",
    "    1. For each question create a sequence Context-Question (with correct model separator)\n",
    "    2. Tokenize the enriched sequences\n",
    "3. Pass the sequence through the model\n",
    "    1. This outputs a range of scores across the entire sequence tokens (question and text), for both the start and end positions.\n",
    "4. Compute the softmax of the result to get probabilities over the tokens.\n",
    "5. Fetch the tokens from the identified start and stop values, convert those tokens to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "da28f5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c73c35244544c179bacac87d63bfde3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a16e98a6c447b882cd205541ef21ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e0810f1eaf4cdead93574849e622c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6560676e1594a45aa7ad025043e4ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c17ae76dcb649a8866c95646de963ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "#1. Initialize tokenizer and model\n",
    "MODEL_PATH = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7ec1313f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How many pretrained models are available in ðŸ¤— Transformers?\n",
      "A: over 32 +\n",
      "Q: What does ðŸ¤— Transformers provide?\n",
      "A: general - purpose architectures\n",
      "Q: ðŸ¤— Transformers provides interoperability between which frameworks?\n",
      "A: tensorflow 2. 0 and pytorch\n",
      "Q: How many languages are available?\n",
      "A: 100 + languages\n"
     ]
    }
   ],
   "source": [
    "# 2. Prepare Context and questions\n",
    "text = r\"\"\"\n",
    "ðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"How many pretrained models are available in ðŸ¤— Transformers?\",\n",
    "    \"What does ðŸ¤— Transformers provide?\",\n",
    "    \"ðŸ¤— Transformers provides interoperability between which frameworks?\",\n",
    "    \"How many languages are available?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    tokens_dict = tokenizer(question, text, add_special_tokens=True, return_tensors='pt')\n",
    "    # store ids for later\n",
    "    input_ids = tokens_dict[\"input_ids\"].tolist()[0]\n",
    "    \n",
    "    outputs = model(**tokens_dict)\n",
    "    #logits\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "    \n",
    "#     probabilities\n",
    "#     answer_start_probabilities = torch.softmax(answer_start_scores)\n",
    "#     answer_end_probabilities = torch.softmax(answer_end_scores)\n",
    "\n",
    "    # predictions\n",
    "    answer_start_token = torch.argmax(answer_start_scores)\n",
    "    answer_end_token = torch.argmax(answer_end_scores)\n",
    "    \n",
    "    answer = tokenizer.decode(input_ids[answer_start_token:answer_end_token + 1])\n",
    "    \n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcf3fe8",
   "metadata": {},
   "source": [
    "## Language Modeling\n",
    "Language modeling is the task of fitting a model to a corpus, which can be domain specific. All popular transformer-based models are trained using a variant of language modeling, e.g. BERT with masked language modeling, GPT-2 with causal language modeling.\n",
    "\n",
    ">Language modeling can be useful outside of pretraining as well, for example to **shift the model distribution** to be domain-specific: using a language model trained over a very large corpus, and then fine-tuning it to a news dataset or on scientific papers e.g. LysandreJik/arxiv-nlp.\n",
    "\n",
    "### Masked Language Modeling\n",
    "Masked language modeling is the task of masking tokens in a sequence with a masking token, and prompting the model to fill that mask with an appropriate token. \n",
    "\n",
    "> This allows the model to attend to both the right context and the left context!\n",
    "\n",
    "#### From Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "30563499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a38142acc8142fe9b079652dd049c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5894456e14c049da9a5d35c5442285dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/316M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb010399fbd4ec6bd9bb9921d4f1ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6050e3aba7bd4e46a91a7538832fa6da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19c6e69736049bebde1b51ca18ec46f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Pipeline\n",
    "unmasker = pipeline('fill-mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e9701d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.17354880273342133,\n",
      "  'sequence': 'HuggingFace is creating a tool thath the community uses to '\n",
      "              'solve NLP tasks',\n",
      "  'token': 3944,\n",
      "  'token_str': ' tool'},\n",
      " {'score': 0.09451553970575333,\n",
      "  'sequence': 'HuggingFace is creating a framework thath the community uses to '\n",
      "              'solve NLP tasks',\n",
      "  'token': 7208,\n",
      "  'token_str': ' framework'},\n",
      " {'score': 0.055287063121795654,\n",
      "  'sequence': 'HuggingFace is creating a bot thath the community uses to solve '\n",
      "              'NLP tasks',\n",
      "  'token': 14084,\n",
      "  'token_str': ' bot'},\n",
      " {'score': 0.04934253543615341,\n",
      "  'sequence': 'HuggingFace is creating a library thath the community uses to '\n",
      "              'solve NLP tasks',\n",
      "  'token': 5560,\n",
      "  'token_str': ' library'},\n",
      " {'score': 0.04610683396458626,\n",
      "  'sequence': 'HuggingFace is creating a plugin thath the community uses to '\n",
      "              'solve NLP tasks',\n",
      "  'token': 43201,\n",
      "  'token_str': ' plugin'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(unmasker(f\"HuggingFace is creating a {unmasker.tokenizer.mask_token} thath the community uses to solve NLP tasks\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be53555",
   "metadata": {},
   "source": [
    "#### From AutoModel\n",
    "\n",
    "1. Instantiate tokenizer and model\n",
    "2. Create a sentence with `tokenizer.mask_token` instead of a word\n",
    "3. Encode the sequence\n",
    "4. Pass through the model\n",
    "    1. The resulting logits can be used to compute the predicted token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e17b33b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "189e890cf5c743ab987d3fb4f865ac32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d392de0d004150807ce3d22758d5dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1f546d2fb641289ac8a5039f0bbffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51951e51a0a9430ca9bd9a5810995ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b0fe9725ef4724836e28b360f55611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/251M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "MODEL_PATH = 'distilbert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c65edd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help [MASK] our carbon footprint.\n",
      "\n",
      " TOP-5 predictions:\n",
      "\t-reduce \tp=71.16%\n",
      "\t-increase \tp=4.62%\n",
      "\t-decrease \tp=3.26%\n",
      "\t-offset \tp=1.88%\n",
      "\t-improve \tp=1.77%\n",
      "\n",
      "Final:\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help reduce our carbon footprint.\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Distilled models are smaller than the models they mimic. Using them instead of the large \" \\\n",
    "f\"versions would help {tokenizer.mask_token} our carbon footprint.\"\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors='pt')\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "# retrieve masked token\n",
    "mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = logits[0, mask_token_index, :]\n",
    "probabilities = torch.softmax(mask_token_logits, dim=1)\n",
    "\n",
    "k = 5\n",
    "print(sequence)\n",
    "print(f'\\n TOP-{k} predictions:')\n",
    "\n",
    "# topk return the values and the indices\n",
    "top_5_tokens = torch.topk(probabilities, k, dim=1)\n",
    "for prob, token in zip(top_5_tokens.values[0], top_5_tokens.indices[0]):\n",
    "    print(f'\\t-{tokenizer.decode(token)} \\tp={prob*100:.2f}%')\n",
    "    \n",
    "\n",
    "best_token = top_5_tokens.indices[0, 0]\n",
    "print(\"\\nFinal:\")\n",
    "print(sequence.replace(tokenizer.mask_token, tokenizer.decode(best_token)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "43a708c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 28996])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape   #[batch_size, seq_size, vocab_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1a4787",
   "metadata": {},
   "source": [
    "### Causal Language Modeling (Next Token Prediction)\n",
    "Causal language modeling is the task of predicting the token following a sequence of tokens. In this situation, the model only attends to the left context (tokens on the left of the mask). Such a training is particularly interesting for generation tasks\n",
    "\n",
    "> Usually, the next token is predicted by sampling from the logits of the last hidden state the model produces from the input sequence.\n",
    "\n",
    "1. Initialize tokenizer and model\n",
    "2. Prepare (encode) a sequence and pass through the model\n",
    "3. Generate next token\n",
    "    1. Retrieve the last hidden state (last token logits)\n",
    "    2. Filtering\n",
    "    3. Sample\n",
    "    4. Append token to sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a2876d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, top_k_top_p_filtering\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3778bada",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = f\"Hugging Face is based in DUMBO, New York City, and\"\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b1bee902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face is based in DUMBO, New York City, and\n",
      "Hugging Face is based in DUMBO, New York City, and launched\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# retrieve last hidden state\n",
    "next_token_logits = outputs['logits'][:,-1,:]\n",
    "\n",
    "# filter\n",
    "next_token_logits = top_k_top_p_filtering(next_token_logits, \n",
    "                                          top_k=50, \n",
    "                                          top_p=1.0)\n",
    "\n",
    "# sample\n",
    "probs = torch.softmax(next_token_logits, dim=1)\n",
    "next_token = torch.multinomial(probs, 1)\n",
    "generated = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "generated_string = tokenizer.decode(generated.tolist()[0])\n",
    "print(sequence)\n",
    "print(generated_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0015590",
   "metadata": {},
   "source": [
    "### Text Generation\n",
    "In **text generation** (a.k.a open-ended text generation) the goal is to create a coherent portion of text that is a continuation from the given context. The following example shows how GPT-2 can be used in pipelines to generate text. As a default all models apply Top-K sampling when used in pipelines, as configured in their respective configurations (see gpt-2 config for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "00c5a530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 (https://huggingface.co/gpt2)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'As far as I am concerned, I will be the first to admit that I am not a fan of the idea of a \"free market.\" I think that the idea of a free market is a bit of a stretch. I think that the idea'}]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "text_generator = pipeline('text-generation')\n",
    "text_generator(\"As far as I am concerned, I will\",\n",
    "              max_length=50, \n",
    "              do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2c58eff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"As far as I am concerned, I will do my utmost to protect anyone and everyone in the state from having any personal belongings taken away from them.\\n\\n\\nLet's be clear for the record. The purpose of this policy, as described in these\"}]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generator(\"As far as I am concerned, I will\",\n",
    "              max_length=50, \n",
    "              do_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d1c7d0",
   "metadata": {},
   "source": [
    "Without sampling the model can be easily stuck on a repetition loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f152f8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'As far as I am concerned, I will be the first to admit that I am not a fan of the idea of a \"free market.\" I think that the idea of a free market is a bit of a stretch. I think that the idea of a free market is a bit of a stretch. I think that the idea of a free market is a bit of a stretch. I think that the idea of a free market is a bit of a stretch. I think that the idea of a'}]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generator(\"As far as I am concerned, I will\",\n",
    "              max_length=100, \n",
    "              do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5fd82145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"As far as I am concerned, I will stick to our previous point of view as a reason to support people in this situation, rather than to be left alone. So, why can't you guys let everyone off with an option like that?\\n\\n\\nIf you put your concerns into place then they can all be resolved right?\\n\\n\\nI think we should let people know how we feel about it, we all have our views on it and we are all in the same boat here.\\n\\n\\n\"}]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generator(\"As far as I am concerned, I will\",\n",
    "              max_length=100, \n",
    "              do_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721e3bde",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "63f710b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2c2a9873294f07bc445cc9051361ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338a0ab23908463a9527bd05d1b29cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/779k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c604c47af942bfadca695a0366bc4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba88df412524aec9778d16a284c9430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_PATH = 'xlnet-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "438cb0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today the weather is really nice and I am planning on heading out for lunch. After I get the house clean and clean, I am going to drive around the neighbourhood to see the neighbourhood's parks. After I have gotten up, I plan to drive around the neighborhood and see a lot of local parks. During my drive, I will take a break. (I'm really nervous, I'm about to\n"
     ]
    }
   ],
   "source": [
    "# Padding text helps XLNet with short prompts - proposed by Aman Rusia in https://github.com/rusiaaman/XLNet-gen#methodology\n",
    "PADDING_TEXT = \"\"\"In 1991, the remains of Russian Tsar Nicholas II and his family\n",
    "(except for Alexei and Maria) are discovered.\n",
    "The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\n",
    "remainder of the story. 1883 Western Siberia,\n",
    "a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
    "Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
    "father initially slaps him for making such an accusation, Rasputin watches as the\n",
    "man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
    "the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
    "with people, even a bishop, begging for his blessing. <eod> </s> <eos>\"\"\"\n",
    "\n",
    "prompt = \"Today the weather is really nice and I am planning on \"\n",
    "inputs = tokenizer(PADDING_TEXT + prompt, add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "prompt_length = len(tokenizer.decode(inputs[0]))\n",
    "outputs = model.generate(inputs, max_length=250, do_sample=True, top_p=0.95, top_k=60)\n",
    "generated = prompt + tokenizer.decode(outputs[0])[prompt_length+1:]\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ffb80a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
